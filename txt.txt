_____ SPARK SS _______
Spark Structured Streaming:
Spark Structured Streaming is built on top of the Spark SQL engine which processes streaming data. All the transformations can be performed the same way on the streaming data as done on static data using Spark Structured Streaming. And whatever transformations we perform will be run incrementally and continuously on the streaming data as they arrive. Spark SQL engine will take care of running the transformations on the streaming data.

____ WATERMARK _______
Defines an event time watermark for this DataFrame. A watermark tracks a point in time before which we assume no more late data is going to arrive.
____ FOR EACH ________
Execution semantics When the streaming query is started, Spark calls the function or the object’s methods in the following way:

A single copy of this object is responsible for all the data generated by a single task in a query. In other words, one instance is responsible for processing one partition of the data generated in a distributed manner.

This object must be serializable, because each task will get a fresh serialized-deserialized copy of the provided object. Hence, it is strongly recommended that any initialization for writing data (for example. opening a connection or starting a transaction) is done after the open() method has been called, which signifies that the task is ready to generate data.

The lifecycle of the methods are as follows:

For each partition with partition_id:

For each batch/epoch of streaming data with epoch_id:

Method open(partitionId, epochId) is called.

If open(…) returns true, for each row in the partition and batch/epoch, method process(row) is called.

Method close(error) is called with error (if any) seen while processing rows.

The close() method (if it exists) is called if an open() method exists and returns successfully (irrespective of the return value), except if the JVM or Python process crashes in the middle.

Note: Spark does not guarantee same output for (partitionId, epochId), so deduplication cannot be achieved with (partitionId, epochId). e.g. source provides different number of partitions for some reasons, Spark optimization changes number of partitions, etc. See SPARK-28650 for more details. If you need deduplication on output, try out foreachBatch instead.

