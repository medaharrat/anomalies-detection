# -*- coding: utf-8 -*-
"""Live Data Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gPAW-92ALUElnLBvGnghU9ywmh3Mk-HM
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.core.pylabtools import figsize
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import DBSCAN

# %matplotlib inline

class AnomalyDetection:
  def __init__(self):
    pass 

  def svm(self,data):
    svm=OneClassSVM()
    sv=svm.fit_predict(data)
    labels=sv.labels_
    return labels

  def lof(self,data):
    lof=LocalOutlierFactor(n_neighbors=10)
    l=lof.fit_predict(data)
    labels=l.labels_
    return labels

  def dbscan(self,data):
    dbb=DBSCAN(eps=5,min_samples=5)
    dbb.fit_predict(data)
    labels=dbb.labels_
    return labels
    
  def isolotion_tree(self,data):
    iso=IsolationForest(contamination='auto',random_state=42)
    isol=iso.fit_predict(data)
    labels=isol.labels_
    return labels

data=pd.DataFrame()

ad=AnomalyDetection()
while (True):  # Condition: While the data consumes this loop continue to execute
  new_data=pd.read_json("New data")
  data.append(new_data)
  labels=ad.dbscan(data.drop("Time",axis=1))