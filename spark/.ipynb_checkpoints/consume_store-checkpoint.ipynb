{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352858b0",
   "metadata": {},
   "source": [
    "### Install and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9b7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: influxdb-client in /opt/conda/lib/python3.9/site-packages (1.24.0)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.9/site-packages (from influxdb-client) (2021.10.8)\r\n",
      "Requirement already satisfied: rx>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from influxdb-client) (3.2.0)\r\n",
      "Requirement already satisfied: pytz>=2019.1 in /opt/conda/lib/python3.9/site-packages (from influxdb-client) (2021.3)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.9/site-packages (from influxdb-client) (59.2.0)\r\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.9/site-packages (from influxdb-client) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/conda/lib/python3.9/site-packages (from influxdb-client) (1.26.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.9/site-packages (from influxdb-client) (2.8.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install influxdb-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289a5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14233324",
   "metadata": {},
   "source": [
    "### Spark Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8490776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-20239031-5d56-4b2e-93d2-59374dbe30e0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1399ms :: artifacts dl 68ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-20239031-5d56-4b2e-93d2-59374dbe30e0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/39ms)\n",
      "21/12/16 17:23:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df30acf",
   "metadata": {},
   "source": [
    "### Define an input stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce384850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema for the data frame\n",
    "cols = ['Time',\n",
    " 'HMI_FIT101', 'HMI_LIT101', 'HMI_AIT201', 'HMI_AIT202', 'HMI_AIT203', 'HMI_FIT201',\n",
    " 'HMI_DPIT301', 'HMI_FIT301', 'HMI_LIT301', 'HMI_AIT401', 'HMI_AIT402', 'HMI_FIT401', \n",
    " 'HMI_LIT401', 'HMI_AIT501', 'HMI_AIT502', 'HMI_AIT503', 'HMI_AIT504', 'HMI_FIT501',\n",
    " 'HMI_FIT502', 'HMI_FIT503', 'HMI_FIT504', 'HMI_PIT501', 'HMI_PIT502', 'HMI_PIT503', \n",
    " 'HMI_FIT601', 'HMI_MV101', 'HMI_P101', 'HMI_P102', 'HMI_MV201', 'HMI_P201', 'HMI_P202',\n",
    " 'HMI_P203', 'HMI_P204', 'HMI_P205', 'HMI_P206','HMI_MV301', 'HMI_MV302', 'HMI_MV303',\n",
    " 'HMI_MV304', 'HMI_P301', 'HMI_P302', 'HMI_P401', 'HMI_P402', 'HMI_P403', 'HMI_P404',\n",
    " 'HMI_P501', 'HMI_P502', 'HMI_P601', 'HMI_P602', 'HMI_P603', 'HMI_UV401']\n",
    "\n",
    "fields = [StructField(col_name, StringType(), True) for col_name in cols]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014eb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read stream from json and fit schema\n",
    "inputStream = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"SWAT\")\\\n",
    "    .load()\n",
    "\n",
    "inputStream = inputStream.select(col(\"value\").cast(\"string\").alias(\"data\"))\n",
    "# inputStream = inputStream.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "inputStream.printSchema()\n",
    "# data = inputStream.select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7e96d",
   "metadata": {},
   "source": [
    "### Batch Processing and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de66f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca(data):\n",
    "    sc = StandardScaler()\n",
    "    data = sc.fit_transform(data)\n",
    "    pca = PCA(n_components = .95)\n",
    "    pca.fit(data)\n",
    "    reduced = pca.transform(data)\n",
    "    principal_components = pd.DataFrame(data = reduced, columns=[f\"P{col + 1}\" for col in range(reduced.shape[1])])\n",
    "    return principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1059f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class InfluxDBWriter:\n",
    "    def __init__(self, cloud=False):\n",
    "        self.url = \"http://influxdb:8086\"\n",
    "        self.token = \"iJHZR-dq4I5LIpFZCc5bTUHx-I7dyz29ZTO-B4W5DpU4mhPVDFg-aAb2jK4Vz1C6n0DDb6ddA-bJ3EZAanAOUw==\"\n",
    "        self.org = \"primary\"\n",
    "        self.bucket = \"swat\"\n",
    "        if cloud: # Connect to InfluxDB Cloud\n",
    "            self.client = InfluxDBClient(\n",
    "                url=\"https://westeurope-1.azure.cloud2.influxdata.com\", \n",
    "                token=\"iJHZR-dq4I5LIgFZCc5jTUNx-I7dyz29ZTO-B4W5DpU4mhPVDFg-aAb2jK4Vz1C6n0DDb6ddA-bJ3EZAanAOUw==\", \n",
    "                org=\"ahr9oi@inf.elte.hu\"\n",
    "            )\n",
    "        else: # Connect to a local instance of InfluxDB\n",
    "            self.client = InfluxDBClient(url=self.url, token=self.token, org=self.org)\n",
    "        # Create a writer API\n",
    "        self.write_api = self.client.write_api()\n",
    "\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        print(\"Opened %d, %d\" % (partition_id, epoch_id))\n",
    "        return True\n",
    "    \n",
    "    def preprocess(self, row):\n",
    "        row_dict = json.loads(row)\n",
    "        for key, val in row_dict.items():\n",
    "            if key == 'AIT401' or key == 'P102' or key == 'P202' or key == 'P204'\\\n",
    "            or key == 'P206' or key == 'P302' or key == 'P402' or key == 'P403'\\\n",
    "            or key == 'P404' or key == 'P502' or key == 'P601' or key == 'P603':\n",
    "                row_dict.pop(key)\n",
    "                \n",
    "        row_list = [val for key, val in row_dict if key != 'Time']\n",
    "        principal_comps = pca(row_list)\n",
    "        return principal_comps\n",
    "\n",
    "    def process(self, row):\n",
    "        try:\n",
    "            preprocessed = self.preprocess(row[\"data\"])\n",
    "            print(len(preprocessed))\n",
    "            #self.write_api.write(bucket=self.bucket, org=self.org, record=self._row_to_point(row[\"data\"]))\n",
    "            #print(f\"> Inserted into {self.bucket} bucket\")\n",
    "        except Exception as ex:\n",
    "            print(f\"[x] Error {str(ex)}\")\n",
    "\n",
    "    def _row_to_point(self, row):\n",
    "        # Load to dictionary\n",
    "        row_dict = json.loads(row)\n",
    "        # String to timestamp\n",
    "        # timestamp = datetime.datetime.strptime(row_dict[\"Time\"], \"%d/%m/%Y %H:%M:%S.%f %p\")\n",
    "        # Create a data point\n",
    "        point = Point(\"data\").tag(\"anomaly\", \"false\")\n",
    "        # Add fields to the point\n",
    "        for key, val in row_dict.items():\n",
    "            if key != 'Time':\n",
    "                point.field(key, float(val))\n",
    "        # Add timestamp\n",
    "        point.time(datetime.now())\n",
    "        return point\n",
    "    \n",
    "    def _is_anomaly(self, row):\n",
    "        row_dict = json.loads(row)\n",
    "        data_row = []\n",
    "        # Process dict to array of data\n",
    "        for key, val in row_dict.items():\n",
    "            if key != 'Time':\n",
    "                data_row.append(val)\n",
    "        # Polynomial SVC on Isolation Trees model\n",
    "        svc = pickle.load(open('./models/svc_poly.pickle', 'rb'))\n",
    "        # Detect anomalies\n",
    "        preds = pca(np.asarray(data_row).reshape(-1, 1))\n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a227a77",
   "metadata": {},
   "source": [
    "### Delete records from a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ae8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE RECORDS\n",
    "#from datetime import datetime\n",
    "\n",
    "#client = InfluxDBClient(url=\"http://influxdb:8086\", token=\"iJHZR-dq4I5LIpFZCc5bTUHx-I7dyz29ZTO-B4W5DpU4mhPVDFg-aAb2jK4Vz1C6n0DDb6ddA-bJ3EZAanAOUw==\", org=\"primary\")\n",
    "#delete_api = client.delete_api()\n",
    "#start = \"1970-01-01T00:00:00Z\"\n",
    "#measurement = \"mem\"\n",
    "#delete_api.delete(start, datetime.now(), f'_measurement=\"{measurement}\"', bucket=\"primary\", org=\"primary\")\n",
    "#print(\"> Deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737d124",
   "metadata": {},
   "source": [
    "### Read stream and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e372f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opened 0, 0                                                         (0 + 1) / 1]\n",
      "Opened 0, 1                                                         (0 + 1) / 1]\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "Opened 0, 2                                                                     \n",
      "51\n",
      "Opened 0, 3\n",
      "51\n",
      "Opened 0, 4\n",
      "51tage 4:>                                                          (0 + 1) / 1]\n",
      "Opened 0, 5                                                         (0 + 1) / 1]\n",
      "51\n",
      "Opened 0, 6                                                                     \n",
      "51\n",
      "Opened 0, 7                                                         (0 + 1) / 1]\n",
      "51\n",
      "Opened 0, 8                                                                     \n",
      "51\n",
      "Opened 0, 9\n",
      "51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18821/2982736707.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m          .start())\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. Output data: show result in the console\n",
    "query = (inputStream\n",
    "         .writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .foreach(InfluxDBWriter())\n",
    "         .start())\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5ae5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
